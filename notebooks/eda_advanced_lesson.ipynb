{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Advanced Lesson "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance and Correlation\n",
    "\n",
    "Covariance and correlation are two mathematical concepts which are commonly used in statistics. They are used to determine the relationship between two variables. The covariance is used to measure the linear relationship between two variables. On the other hand, the correlation is used to measure both the strength and direction of the _linear relationship_ between two variables.\n",
    "\n",
    "Covariance is a measure of how much two random variables vary together. It’s similar to variance, but where variance tells you how a single variable varies, covariance tells you how two variables vary together.\n",
    "\n",
    "Correlation (coefficient) is a _normalized_ measure of covariance that is easier to understand, as it provides quantitative measurements of the statistical dependence between two random variables. The correlation coefficient is a value that indicates the strength of the relationship between variables. The coefficient can take any values from -1 to 1. The interpretations of the values are:\n",
    "\n",
    "- **-1**: Perfect negative linear correlation\n",
    "- **-0.8**: Strong negative linear correlation\n",
    "- **-0.5**: Moderate negative linear correlation\n",
    "- **-0.2**: Weak negative linear correlation\n",
    "- **0**: No linear correlation\n",
    "- **0.2**: Weak positive linear correlation\n",
    "- **0.5**: Moderate positive linear correlation\n",
    "- **0.8**: Strong positive linear correlation\n",
    "- **1**: Perfect positive linear correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll use DataFrames of stock prices and volumes obtained from Yahoo! Finance available in binary Python pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_pickle(\"../data/yahoo_price.pkl\")\n",
    "volume = pd.read_pickle(\"../data/yahoo_volume.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>27.990226</td>\n",
       "      <td>313.062468</td>\n",
       "      <td>113.304536</td>\n",
       "      <td>25.884104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>28.038618</td>\n",
       "      <td>311.683844</td>\n",
       "      <td>111.935822</td>\n",
       "      <td>25.892466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>27.592626</td>\n",
       "      <td>303.826685</td>\n",
       "      <td>111.208683</td>\n",
       "      <td>25.733566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>27.541619</td>\n",
       "      <td>296.753749</td>\n",
       "      <td>110.823732</td>\n",
       "      <td>25.465944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>27.724725</td>\n",
       "      <td>300.709808</td>\n",
       "      <td>111.935822</td>\n",
       "      <td>25.641571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-17</th>\n",
       "      <td>117.550003</td>\n",
       "      <td>779.960022</td>\n",
       "      <td>154.770004</td>\n",
       "      <td>57.220001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-18</th>\n",
       "      <td>117.470001</td>\n",
       "      <td>795.260010</td>\n",
       "      <td>150.720001</td>\n",
       "      <td>57.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-19</th>\n",
       "      <td>117.120003</td>\n",
       "      <td>801.500000</td>\n",
       "      <td>151.259995</td>\n",
       "      <td>57.529999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-20</th>\n",
       "      <td>117.059998</td>\n",
       "      <td>796.969971</td>\n",
       "      <td>151.520004</td>\n",
       "      <td>57.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-21</th>\n",
       "      <td>116.599998</td>\n",
       "      <td>799.369995</td>\n",
       "      <td>149.630005</td>\n",
       "      <td>59.660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1714 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  AAPL        GOOG         IBM       MSFT\n",
       "Date                                                     \n",
       "2010-01-04   27.990226  313.062468  113.304536  25.884104\n",
       "2010-01-05   28.038618  311.683844  111.935822  25.892466\n",
       "2010-01-06   27.592626  303.826685  111.208683  25.733566\n",
       "2010-01-07   27.541619  296.753749  110.823732  25.465944\n",
       "2010-01-08   27.724725  300.709808  111.935822  25.641571\n",
       "...                ...         ...         ...        ...\n",
       "2016-10-17  117.550003  779.960022  154.770004  57.220001\n",
       "2016-10-18  117.470001  795.260010  150.720001  57.660000\n",
       "2016-10-19  117.120003  801.500000  151.259995  57.529999\n",
       "2016-10-20  117.059998  796.969971  151.520004  57.250000\n",
       "2016-10-21  116.599998  799.369995  149.630005  59.660000\n",
       "\n",
       "[1714 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>123432400</td>\n",
       "      <td>3927000</td>\n",
       "      <td>6155300</td>\n",
       "      <td>38409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>150476200</td>\n",
       "      <td>6031900</td>\n",
       "      <td>6841400</td>\n",
       "      <td>49749600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>138040000</td>\n",
       "      <td>7987100</td>\n",
       "      <td>5605300</td>\n",
       "      <td>58182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>119282800</td>\n",
       "      <td>12876600</td>\n",
       "      <td>5840600</td>\n",
       "      <td>50559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>111902700</td>\n",
       "      <td>9483900</td>\n",
       "      <td>4197200</td>\n",
       "      <td>51197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-17</th>\n",
       "      <td>23624900</td>\n",
       "      <td>1089500</td>\n",
       "      <td>5890400</td>\n",
       "      <td>23830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-18</th>\n",
       "      <td>24553500</td>\n",
       "      <td>1995600</td>\n",
       "      <td>12770600</td>\n",
       "      <td>19149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-19</th>\n",
       "      <td>20034600</td>\n",
       "      <td>116600</td>\n",
       "      <td>4632900</td>\n",
       "      <td>22878400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-20</th>\n",
       "      <td>24125800</td>\n",
       "      <td>1734200</td>\n",
       "      <td>4023100</td>\n",
       "      <td>49455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-21</th>\n",
       "      <td>22384800</td>\n",
       "      <td>1260500</td>\n",
       "      <td>4401900</td>\n",
       "      <td>79974200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1714 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AAPL      GOOG       IBM      MSFT\n",
       "Date                                               \n",
       "2010-01-04  123432400   3927000   6155300  38409100\n",
       "2010-01-05  150476200   6031900   6841400  49749600\n",
       "2010-01-06  138040000   7987100   5605300  58182400\n",
       "2010-01-07  119282800  12876600   5840600  50559700\n",
       "2010-01-08  111902700   9483900   4197200  51197400\n",
       "...               ...       ...       ...       ...\n",
       "2016-10-17   23624900   1089500   5890400  23830000\n",
       "2016-10-18   24553500   1995600  12770600  19149500\n",
       "2016-10-19   20034600    116600   4632900  22878400\n",
       "2016-10-20   24125800   1734200   4023100  49455600\n",
       "2016-10-21   22384800   1260500   4401900  79974200\n",
       "\n",
       "[1714 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute percent changes of the prices using a window function (we will explain window functions in the later section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-10-17</th>\n",
       "      <td>-0.000680</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>-0.003483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-18</th>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.019616</td>\n",
       "      <td>-0.026168</td>\n",
       "      <td>0.007690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-19</th>\n",
       "      <td>-0.002979</td>\n",
       "      <td>0.007846</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>-0.002255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-20</th>\n",
       "      <td>-0.000512</td>\n",
       "      <td>-0.005652</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>-0.004867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-21</th>\n",
       "      <td>-0.003930</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>-0.012474</td>\n",
       "      <td>0.042096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                AAPL      GOOG       IBM      MSFT\n",
       "Date                                              \n",
       "2016-10-17 -0.000680  0.001837  0.002072 -0.003483\n",
       "2016-10-18 -0.000681  0.019616 -0.026168  0.007690\n",
       "2016-10-19 -0.002979  0.007846  0.003583 -0.002255\n",
       "2016-10-20 -0.000512 -0.005652  0.001719 -0.004867\n",
       "2016-10-21 -0.003930  0.003011 -0.012474  0.042096"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns = price.pct_change()\n",
    "\n",
    "returns.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the correlation and covariance between the returns of `MSFT` and `IBM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.870655479703546e-05)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns[\"MSFT\"].cov(returns[\"IBM\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.49976361144151144)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns[\"MSFT\"].corr(returns[\"IBM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get the full (pair-wise) correlation or covariance matrix as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AAPL      GOOG       IBM      MSFT\n",
       "AAPL  0.000277  0.000107  0.000078  0.000095\n",
       "GOOG  0.000107  0.000251  0.000078  0.000108\n",
       "IBM   0.000078  0.000078  0.000146  0.000089\n",
       "MSFT  0.000095  0.000108  0.000089  0.000215"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.cov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>GOOG</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MSFT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407919</td>\n",
       "      <td>0.386817</td>\n",
       "      <td>0.389695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>0.407919</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.405099</td>\n",
       "      <td>0.465919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IBM</th>\n",
       "      <td>0.386817</td>\n",
       "      <td>0.405099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSFT</th>\n",
       "      <td>0.389695</td>\n",
       "      <td>0.465919</td>\n",
       "      <td>0.499764</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AAPL      GOOG       IBM      MSFT\n",
       "AAPL  1.000000  0.407919  0.386817  0.389695\n",
       "GOOG  0.407919  1.000000  0.405099  0.465919\n",
       "IBM   0.386817  0.405099  1.000000  0.499764\n",
       "MSFT  0.389695  0.465919  0.499764  1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compute pair-wise correlations between a DataFrame’s columns or rows with another Series or DataFrame. Passing a Series returns a Series with the correlation value computed for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL    0.386817\n",
       "GOOG    0.405099\n",
       "IBM     1.000000\n",
       "MSFT    0.499764\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.corrwith(returns[\"IBM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a DataFrame computes the correlations of matching column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AAPL   -0.075565\n",
       "GOOG   -0.007067\n",
       "IBM    -0.204849\n",
       "MSFT   -0.092950\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.corrwith(volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis Functions\n",
    "\n",
    "Beyond `.corr()` and `.corrwith()`, pandas provides a rich set of statistical functions for analyzing relationships and distributions in your data. Let's explore the key functions using our stock market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRELATION FUNCTIONS ===\n",
      "\n",
      "1. .corr() - Pairwise correlation matrix:\n",
      "          AAPL      MSFT       IBM\n",
      "AAPL  1.000000  0.389695  0.386817\n",
      "MSFT  0.389695  1.000000  0.499764\n",
      "IBM   0.386817  0.499764  1.000000\n",
      "\n",
      "2. .corrwith() - Correlation with specific series:\n",
      "AAPL    1.000000\n",
      "GOOG    0.407919\n",
      "IBM     0.386817\n",
      "MSFT    0.389695\n",
      "dtype: float64\n",
      "\n",
      "=== COVARIANCE FUNCTIONS ===\n",
      "\n",
      "3. .cov() - Pairwise covariance matrix:\n",
      "          AAPL      MSFT       IBM\n",
      "AAPL  0.000277  0.000095  0.000078\n",
      "MSFT  0.000095  0.000215  0.000089\n",
      "IBM   0.000078  0.000089  0.000146\n",
      "\n",
      "4. Individual covariance between two series:\n",
      "AAPL vs MSFT covariance: 0.000095\n"
     ]
    }
   ],
   "source": [
    "# 1. CORRELATION FUNCTIONS\n",
    "print(\"=== CORRELATION FUNCTIONS ===\")\n",
    "print(\"\\n1. .corr() - Pairwise correlation matrix:\")\n",
    "print(returns[['AAPL', 'MSFT', 'IBM']].corr())\n",
    "\n",
    "print(\"\\n2. .corrwith() - Correlation with specific series:\")\n",
    "print(returns.corrwith(returns['AAPL']))\n",
    "\n",
    "# 3. COVARIANCE FUNCTIONS  \n",
    "print(\"\\n=== COVARIANCE FUNCTIONS ===\")\n",
    "print(\"\\n3. .cov() - Pairwise covariance matrix:\")\n",
    "print(returns[['AAPL', 'MSFT', 'IBM']].cov())\n",
    "\n",
    "print(\"\\n4. Individual covariance between two series:\")\n",
    "print(f\"AAPL vs MSFT covariance: {returns['AAPL'].cov(returns['MSFT']):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DESCRIPTIVE STATISTICS ===\n",
      "\n",
      "5. .describe() - Comprehensive statistical summary:\n",
      "count    1713.000000\n",
      "mean        0.000972\n",
      "std         0.016641\n",
      "min        -0.123558\n",
      "25%        -0.007516\n",
      "50%         0.000886\n",
      "75%         0.010422\n",
      "max         0.088741\n",
      "Name: AAPL, dtype: float64\n",
      "\n",
      "6. Individual statistical measures:\n",
      "Standard deviation: 0.016641\n",
      "Variance: 0.000277\n",
      "Skewness: -0.120540\n",
      "Kurtosis: 4.218746\n",
      "\n",
      "7. Additional measures:\n",
      "Mean: 0.000972\n",
      "Median: 0.000886\n",
      "Min: -0.123558\n",
      "Max: 0.088741\n",
      "Range: 0.212299\n"
     ]
    }
   ],
   "source": [
    "# 5. DESCRIPTIVE STATISTICS FUNCTIONS\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "print(\"\\n5. .describe() - Comprehensive statistical summary:\")\n",
    "print(returns['AAPL'].describe())\n",
    "\n",
    "print(\"\\n6. Individual statistical measures:\")\n",
    "print(f\"Standard deviation: {returns['AAPL'].std():.6f}\")\n",
    "print(f\"Variance: {returns['AAPL'].var():.6f}\")\n",
    "print(f\"Skewness: {returns['AAPL'].skew():.6f}\")\n",
    "print(f\"Kurtosis: {returns['AAPL'].kurtosis():.6f}\")\n",
    "\n",
    "print(\"\\n7. Additional measures:\")\n",
    "print(f\"Mean: {returns['AAPL'].mean():.6f}\")\n",
    "print(f\"Median: {returns['AAPL'].median():.6f}\")\n",
    "print(f\"Min: {returns['AAPL'].min():.6f}\")\n",
    "print(f\"Max: {returns['AAPL'].max():.6f}\")\n",
    "print(f\"Range: {returns['AAPL'].max() - returns['AAPL'].min():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. QUANTILE AND PERCENTILE FUNCTIONS\n",
    "print(\"=== QUANTILES & PERCENTILES ===\")\n",
    "print(\"\\n8. Quantile analysis:\")\n",
    "print(f\"25th percentile: {returns['AAPL'].quantile(0.25):.6f}\")\n",
    "print(f\"75th percentile: {returns['AAPL'].quantile(0.75):.6f}\")\n",
    "print(f\"IQR: {returns['AAPL'].quantile(0.75) - returns['AAPL'].quantile(0.25):.6f}\")\n",
    "\n",
    "print(\"\\n9. Multiple quantiles at once:\")\n",
    "print(returns['AAPL'].quantile([0.1, 0.25, 0.5, 0.75, 0.9]))\n",
    "\n",
    "# 10. ROLLING STATISTICAL FUNCTIONS\n",
    "print(\"\\n=== ROLLING STATISTICS ===\")\n",
    "print(\"\\n10. Rolling statistics (30-day window):\")\n",
    "rolling_stats = returns['AAPL'].rolling(30)\n",
    "print(f\"Rolling mean (last value): {rolling_stats.mean().iloc[-1]:.6f}\")\n",
    "print(f\"Rolling std (last value): {rolling_stats.std().iloc[-1]:.6f}\")\n",
    "print(f\"Rolling correlation with MSFT (last value): {returns['AAPL'].rolling(30).corr(returns['MSFT']).iloc[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Statistical Function Results\n",
    "\n",
    "**Key Interpretations:**\n",
    "\n",
    "- **Correlation (-1 to 1)**: Measures linear relationship strength\n",
    "  - Close to 1: Strong positive relationship \n",
    "  - Close to -1: Strong negative relationship\n",
    "  - Close to 0: Weak linear relationship\n",
    "\n",
    "- **Covariance**: Raw measure of how variables move together (units matter)\n",
    "\n",
    "- **Skewness**: Measures asymmetry of distribution\n",
    "  - Positive: Right tail is longer\n",
    "  - Negative: Left tail is longer\n",
    "  - Close to 0: Roughly symmetric\n",
    "\n",
    "- **Kurtosis**: Measures \"tailedness\" \n",
    "  - High values: Heavy tails (more extreme values)\n",
    "  - Low values: Light tails (fewer extreme values)\n",
    "\n",
    "- **Standard Deviation vs Variance**: Both measure spread, but std dev is in original units\n",
    "\n",
    "> **Practice Question**: Using the functions above, which two stocks show the strongest correlation? What does their covariance tell you that correlation doesn't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical indexing (MultiIndex) allows you to have multiple (two or more) _index levels_ on an axis. It enables \"higher dimensional\" data in a lower dimensional data structure.\n",
    "\n",
    "You create a hierarchical index by simply passing a list of arrays to the index argument of a pandas DataFrame or Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(np.random.uniform(size=9),\n",
    "                 index=[['a', 'a', 'a', 'b', 'b', 'c', 'c', 'd', 'd'],\n",
    "                 [1, 2, 3, 1, 3, 1, 2, 2, 3]])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use _partial indexing_ to select subsets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"b\":\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[[\"b\", \"d\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also select from \"inner\" level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical indexing works on both axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame(np.arange(12).reshape((4, 3)),\n",
    "                        index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\n",
    "                        columns=[['Ohio', 'Ohio', 'Colorado'],\n",
    "                        ['Green', 'Red', 'Green']])\n",
    "\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting names on the axes work as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.index.names = [\"key1\", \"key2\"]\n",
    "frame.columns.names = [\"state\", \"color\"]\n",
    "\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.index.nlevels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial indexing works on columns too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame[\"Ohio\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to rearrange the order of the levels on an axis. The `swaplevel` method will swap the levels in the MultiIndex on a particular axis. The default is to swap the levels on the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.swaplevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.swaplevel(0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also sort by a single level or subset of levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.sort_index(level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Swap the levels on the rows then sort the index by level `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common to use one or more columns from a DataFrame as the row index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame({\"a\": range(7), \"b\": range(7, 0, -1), \n",
    "                      \"c\": [\"one\", \"one\", \"one\", \"two\", \"two\", \"two\", \"two\"], \n",
    "                      \"d\": [0, 1, 2, 0, 1, 2, 3]})\n",
    "\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_index` will return a new DataFrame using one or more of its columns as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2 = frame.set_index([\"c\", \"d\"])\n",
    "\n",
    "frame2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reset_index` does the opposite of `set_index` and turns the index back into a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose to drop the columns when resetting index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Time Data\n",
    "\n",
    "Pandas is oriented towards working with arrays of dates, whether used as an axis index or a column.\n",
    "\n",
    "The `to_datetime` method parses may different kinds of date representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\"2011-07-06 12:00:00\", \"2011-08-06 00:00:00\"]\n",
    "\n",
    "pd.to_datetime(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It uses `NaT` (Not a Time) as null values for datetime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.to_datetime(dates + [None])\n",
    "\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isna(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Python uses the `datetime` module to handle date and time data. Pandas has a `Timestamp` object that is similar to the `datetime` object. Pandas also has a `Timedelta` object that is similar to the `timedelta` object.\n",
    "\n",
    "If you use `datetime` objects as index to a Series or DataFrame, Pandas will automatically convert them to `DatetimeIndex` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7), datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\n",
    "\n",
    "ts = pd.Series(np.random.standard_normal(6), index=dates)\n",
    "\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other Series, arithmetic operations between differently indexed time series automatically align on the dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [::2] selects every second element\n",
    "ts + ts[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DatetimeIndex` is an array of `Timestamp` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can index by passing a `datetime`, `Timestamp` or `string` that is interpretable as a date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[datetime(2011, 1, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[pd.Timestamp(\"2011-01-07\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts[\"2011-01-07\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even specify the year or year-month strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_range generate an array of dates\n",
    "longer_ts = pd.Series(np.random.standard_normal(1000), \n",
    "                      index=pd.date_range(\"2000-01-01\", periods=1000))\n",
    "\n",
    "longer_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_ts[\"2001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_ts[\"2001-05\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_ts[\"2001-05\":]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use `date_range` to generate a Series of random values from 1-31st January 2023. Then slice the Series to return data from 5-15th January."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daterange = pd.date_range(\"2023-01-01\", \"2023-01-31\", periods=31)\n",
    "s = pd.Series(np.random.randn(len(daterange)), index=daterange)\n",
    "s[\"2023-01-05\":\"2023-01-15\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.read_pickle('../data/yahoo_price.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access other attributes like `day_of_week` or `month`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.index.day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price.index.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the datetime is in a column instead of the index, you can use the `dt` accessor to access the datetime properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_reindex = price.reset_index()\n",
    "\n",
    "price_reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_reindex[\"Date\"].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Get the week of year from the date column and create a new column `week_of_year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the week of the year from the date column and create a new column week_of_year\n",
    "price_reindex[\"week_of_year\"] = price_reindex[\"Date\"].dt.isocalendar().week \n",
    "price_reindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, the dates are on business days, if you want to change the frequency to calendar days (known as resampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_resampled = price.resample('D').asfreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_resampled.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to fill the na values with the most recent value, you can use the `.ffill()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_resampled = price.resample('D').ffill()\n",
    "\n",
    "price_resampled.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to resample to a lower frequency (e.g. monthly) you need to provide an aggregation method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_resampled = price.resample('MS').mean()\n",
    "\n",
    "price_resampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more resampling frequencies options, please refer to the official [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)\n",
    "\n",
    "> Resample price to `yearly` (start of year) frequency, use `sum` as aggregation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_resampled = price.resample('YS').sum()\n",
    "price_resampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window functions\n",
    "\n",
    "You can apply functions evaluated over a sliding window using the `rolling` method.\n",
    "\n",
    "For example, to compute the 30-day moving average for Apple price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price[\"AAPL\"].rolling(30).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, rolling functions require all of the values in the window to be non-NA. This behavior can be changed to account for missing data and, especially at the beginning of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price[\"AAPL\"].rolling(30, min_periods=3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compute a 10-day moving average for `GOOG` with a min period of 5 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price[\"GOOG\"].rolling(10, min_periods=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining and Merging Datasets\n",
    "\n",
    "Data can be combined or merged in a number of ways:\n",
    "\n",
    "- `merge`: connects rows in DataFrames based on one or more keys. Equivalent to database `join` operations.\n",
    "- `concat`: concatenates or \"stacks\" together objects along an axis. Equivalent to database `union` operations.\n",
    "- `combine_first`: instance method enables splicing together overlapping data to fill in missing values in one object with values from another. _We went through this in unit 7_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `merge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"a\", \"b\"], \n",
    "                    \"data1\": pd.Series(range(7), dtype=\"Int64\")})\n",
    "\n",
    "df2 = pd.DataFrame({\"key\": [\"a\", \"b\", \"d\"], \n",
    "                    \"data2\": pd.Series(range(3), dtype=\"Int64\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the two dataframes above constitutes a _many-to-one_ join; the data in `df1` has multiple rows labeled `a` and `b`, whereas `df2` has only one row for each value in the key column `key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did not specify which column(s) to join on, `merge` uses the overlapping column names as the keys. It’s a good practice to specify explicitly, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, on=\"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the column names are different in each object, you can specify them separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({\"lkey\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"a\", \"b\"], \n",
    "                    \"data1\": pd.Series(range(7), dtype=\"Int64\")})\n",
    "\n",
    "df4 = pd.DataFrame({\"rkey\": [\"a\", \"b\", \"d\"], \n",
    "                    \"data2\": pd.Series(range(3), dtype=\"Int64\")})\n",
    "\n",
    "pd.merge(df3, df4, left_on=\"lkey\", right_on=\"rkey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default merge type is `inner` join. You can specify the other options- `left, right, outer` via the `how` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df3, df4, left_on=\"lkey\", right_on=\"rkey\", how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a _many-to-many_ join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"], \n",
    "                    \"data1\": pd.Series(range(6), dtype=\"Int64\")})\n",
    "\n",
    "df2 = pd.DataFrame({\"key\": [\"a\", \"b\", \"a\", \"b\", \"d\"], \n",
    "                    \"data2\": pd.Series(range(5), dtype=\"Int64\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there were `three \"b\"` rows in the left DataFrame and `two` in the right one, there are `six \"b\"` rows in the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Merge `df1` and `df2` with a left join. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To merge with multiple keys, pass a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({\"key1\": [\"foo\", \"foo\", \"bar\"], \n",
    "                     \"key2\": [\"one\", \"two\", \"one\"],\n",
    "                     \"lval\": pd.Series([1, 2, 3], dtype='Int64')})\n",
    "\n",
    "right = pd.DataFrame({\"key1\": [\"foo\", \"foo\", \"bar\", \"bar\"],\n",
    "                      \"key2\": [\"one\", \"one\", \"one\", \"two\"],\n",
    "                      \"rval\": pd.Series([4, 5, 6, 7], dtype='Int64')})\n",
    "\n",
    "pd.merge(left, right, on=[\"key1\", \"key2\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are overlapping non-key column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=\"key1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass `suffixes` to specify the strings to append to the overlaping names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on=\"key1\", suffixes=(\"_left\", \"_right\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the merge key(s) is in the index, you can pass `left_index=True` or `right_index=True` to indicate that the index should be used as the merge key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left1 = pd.DataFrame({\"key\": [\"a\", \"b\", \"a\", \"a\", \"b\", \"c\"],\n",
    "                      \"value\": pd.Series(range(6), dtype=\"Int64\")})\n",
    "\n",
    "right1 = pd.DataFrame({\"group_val\": [3.5, 7]}, index=[\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left1, right1, left_on=\"key\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left1, right1, left_on=\"key\", right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame has a `join` method which performs a left join by default. The join key on the right dataframe has to be the index. The join key on the left dataframe can be an index or a column (by specifying the `on` parameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left1.join(right1, on='key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `concat`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can join DataFrames along any axis which is referred to as _concatenation_ or _stacking_. This is akin to database `union` operations, in any \"direction\" (axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([0, 1], index=[\"a\", \"b\"], dtype=\"Int64\")\n",
    "s2 = pd.Series([2, 3, 4], index=[\"c\", \"d\", \"e\"], dtype=\"Int64\")\n",
    "s3 = pd.Series([5, 6], index=[\"f\", \"g\"], dtype=\"Int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `concat` with these objects in a list glues together the values and indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `concat` works along `axis=\"index\"`, producing another Series. If you pass `axis=\"columns\"`, the result will instead be a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2, s3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2, s3], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default behavior of `concat` is union (`outer` join) of the indexes, you can also intersect them by passing `join='inner'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s4 = pd.concat([s1, s3])\n",
    "\n",
    "s4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s4], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s4], axis=\"columns\", join=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When combining Series along axis=\"columns\", pass the `keys` argument for the DataFrame column headers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2, s3], axis=\"columns\", keys=[\"one\", \"two\", \"three\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Concat `s1`, `s2` and `s3` along index and pass `keys=[\"one\", \"two\", \"three\"]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For DataFrames, it will become a hierarchical index instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.arange(6).reshape(3, 2), index=[\"a\", \"b\", \"c\"],\n",
    "                   columns=[\"one\", \"two\"])\n",
    "\n",
    "df2 = pd.DataFrame(5 + np.arange(4).reshape(2, 2), index=[\"a\", \"c\"], \n",
    "                   columns=[\"three\", \"four\"])               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], axis=\"columns\", keys=[\"level1\", \"level2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the index does not contain any relevant data, and you want to avoid concatenating based on indexes, you can pass the `ignore_index=True` argument, this will assign a new default index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(np.random.standard_normal((3, 4)), \n",
    "                   columns=[\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "df2 = pd.DataFrame(np.random.standard_normal((2, 3)), \n",
    "                   columns=[\"b\", \"d\", \"a\"])\n",
    "\n",
    "pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Concat `df1` and `df2` on the column axis but ignore the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping and Pivoting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping or pivoting dataframes refers to the process of changing the layout of a dataframe. This is useful when you want to change the granularity of your data or when you want to convert a _wide_ dataframe into a _long_ dataframe or vice versa.\n",
    "\n",
    "### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.arange(6).reshape((2, 3)), \n",
    "                    index=pd.Index([\"Ohio\", \"Colorado\"], name=\"state\"),\n",
    "                    columns=pd.Index([\"one\", \"two\", \"three\"], name=\"number\"))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `stack` method pivots the columns into rows, producing a Series with a MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data.stack()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a hierarchically indexed Series, you can rearrange the data back into a DataFrame with `unstack` , which pivots rows into columns.\n",
    "\n",
    "By default, the innermost level is unstacked (same with stack)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can unstack a different level by passing a level number or name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.unstack(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or just stating the name of the level\n",
    "result.unstack(level=\"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you unstack in a DataFrame, the level unstacked becomes the lowest level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"left\": result, \"right\": result + 5},\n",
    "                  columns=pd.Index([\"left\", \"right\"], name=\"side\"))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unstack(level=\"state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unstack(level=\"state\").stack(level=\"side\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting between \"Wide\" and \"Long\" Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long format and wide format are two common ways of organizing data in the context of databases, spreadsheets, or data analysis. They refer to the arrangement of data rows and columns.\n",
    "\n",
    "1. Long format\n",
    "\n",
    "Each row typically represents a single observation or entry, and each column contains variables or attributes related to that observation. This format is also known as \"tidy data\" or \"normalized data.\"\n",
    "\n",
    "Example:\n",
    "\n",
    "| Year | Country | Population |\n",
    "| ---- | ------- | ---------- |\n",
    "| 2019 | SG      | 5.7        |\n",
    "| 2019 | MY      | 31.5       |\n",
    "| 2019 | TH      | 69.8       |\n",
    "| 2020 | SG      | 5.7        |\n",
    "| 2020 | MY      | 32.7       |\n",
    "| 2020 | TH      | 69.8       |\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- It is easier to handle and analyze structured data with different attributes.\n",
    "- Efficient storage for sparse data, as it avoids repeating column headers.\n",
    "\n",
    "2. Wide format\n",
    "\n",
    "Each row contains multiple observations or entries, and each column contains variables or attributes related to that observation.\n",
    "\n",
    "Example:\n",
    "\n",
    "| Year | SG   | MY   | TH   |\n",
    "| ---- | ---- | ---- | ---- |\n",
    "| 2019 | 5.7  | 31.5 | 69.8 |\n",
    "| 2020 | 5.7  | 32.7 | 69.8 |\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Easier to read and understand when the number of variables is limited.\n",
    "- Suitable for simple summary statistics and basic analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_reindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \"pivot\" a table from a \"wide\" format to a \"long\" format using the `melt` function.\n",
    "\n",
    "The `date` column is the group indicator, while the other columns are data values. We need to indicate the group indicator(s):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted = pd.melt(price_reindex, id_vars=\"Date\")\n",
    "\n",
    "melted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rerun `melt` and pass arguments such that the new columns are named `Company` and `Price` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `pivot`, we can reshape back to the original layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped = melted.pivot(index='Date', columns='variable', values='value')\n",
    "\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aggregation is the process of grouping data together and performing calculations on them. It is equivalent to the `GROUP BY` clause in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"key1\" : [\"a\", \"a\", None, \"b\", \"b\", \"a\", None], \n",
    "                   \"key2\" : pd.Series([1, 2, 1, 2, 1, None, 1], dtype=\"Int64\"),\n",
    "                   \"data1\" : np.random.standard_normal(7), \n",
    "                   \"data2\" : np.random.standard_normal(7)})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to compute the mean for each unique value in `key1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"key1\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not make sense to compute the mean for `key2` since it is a categorical variable and also serves as a key.\n",
    "\n",
    "We can select the numeric columns to compute the mean for (after the `groupby` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"key1\")[[\"data1\", \"data2\"]].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following also works, since the returned result is a DataFrame, however it is less efficient as the selection/subset happens after the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"key1\").mean()[[\"data1\", \"data2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can group by more than 1 column. There is a useful GroupBy method `size` which returns a Series containing group sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['key1', 'key2']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also group by other `Series`/`array`/`list` with the same length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array([\"OH\", \"CA\", \"CA\", \"OH\", \"OH\", \"CA\", \"OH\"])\n",
    "years = [2005, 2005, 2006, 2005, 2006, 2005, 2006]\n",
    "\n",
    "df[\"data1\"].groupby([states, years]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For built-in aggregation methods in pandas, refer to the [documentation](https://pandas.pydata.org/docs/user_guide/groupby.html#built-in-aggregation-methods).\n",
    "\n",
    "> Group by `key1` and `key2` and compute the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own aggregation functions, pass any function that aggregates an array to the `aggregate` method or its short alias `agg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_to_peak(arr):\n",
    "    return arr.max() - arr.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df.groupby(\"key1\")\n",
    "\n",
    "grouped.agg(peak_to_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"key1\").agg(peak_to_peak)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass a list of functions, or function names (for built-in functions) to `aggregate`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg([peak_to_peak, \"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply\n",
    "\n",
    "The most general-purpose GroupBy method is `apply`, which splits the object being manipulated into pieces, invokes the passed function on each piece, and then concatenates the pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = pd.read_csv(\"../data/tips.csv\")\n",
    "\n",
    "tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column with the tip percentage\n",
    "\n",
    "tips[\"tip_pct\"] = tips[\"tip\"] / tips[\"total_bill\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to select the top five `tip_pct` values by group. First, write a function that selects the rows with the largest values in a particular column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top(df, n=5, column=\"tip_pct\"): # default is 5 rows, you can specify n and column\n",
    "    return df.sort_values(column, ascending=False)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top(tips, n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then `apply` this function by different groups using `groupby`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby(\"smoker\").apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass the arguments to the function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.groupby([\"smoker\", \"day\"]).apply(top, n=2, column=\"total_bill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Apply the function on `day` and `time` group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create a function that selects the bottom five `tip_pct` values.\n",
    ">\n",
    "> Then apply it on `smoker` group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "You can also transform your data using the `transform` method. It is similar to `apply` but imposes more restrictions on the type of function you can use. The function must:\n",
    "\n",
    "- Produce a scalar value to be broadcast to the shape of the group chunk, or\n",
    "- Return an object that is the same shape as the group chunk\n",
    "- Not mutate its input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['a', 'b', 'c'] * 4, 'value': np.arange(12.)})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = df.groupby('key')['value']\n",
    "\n",
    "g.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transform` produce a Series of the same shape as `df['value']` but with values replaced by the average grouped by `key`.\n",
    "\n",
    "We can pass a function or function name (for built-in aggregation) to `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.transform(lambda g: g.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_two(group):\n",
    "    return group * 2\n",
    "\n",
    "g.transform(times_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common transformation in data analytics / science is _standardization_ or _standard scaling_. This is where we transform the data to have a mean of 0 and a standard deviation of 1. It is also known as _z-score normalization_.\n",
    "\n",
    "The formula for standard scaling is:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $x$ is the value, $\\mu$ is the mean, and $\\sigma$ is the standard deviation.\n",
    "\n",
    "We can achieve this using `transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "g.transform(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the following works too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = (df['value'] - g.transform('mean')) / g.transform('std')\n",
    "\n",
    "standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables and Cross-Tabulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot table is a data summarization tool that is used in the context of data processing. Pivot tables are used to summarize, sort, reorganize, group, count, total or average data. It allows its users to transform columns into rows and rows into columns. It allows grouping by any data field.\n",
    "\n",
    "In pandas, you can use the `pivot_table` method which is made possible through the `groupby` and `reshape` operations utilizing hierarchical indexing. In addition, `pivot_table` can add partial totals, also known as _margins_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default aggregation for `pivot_table` is mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"day\", \"smoker\"], values=[\"size\", \"tip\", \"tip_pct\", \"total_bill\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can put `smoker` in the table columns and `time` and `day` in the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"time\", \"day\"], columns=\"smoker\", \n",
    "                 values=[\"tip_pct\", \"size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add partial totals by passing `margins=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"time\", \"day\"], columns=\"smoker\", \n",
    "                 values=[\"tip_pct\", \"size\"], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use other aggregation functions, pass it to the `aggfunc` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"time\", \"smoker\"], columns=\"day\", \n",
    "                 values=\"tip_pct\", aggfunc=len, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `fill_value` to fill missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.pivot_table(index=[\"time\", \"smoker\"], columns=\"day\", \n",
    "                 values=\"tip_pct\", aggfunc=len, margins=True, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compute the sum of `tip` in a pivot table with `day` and `time` in the rows and `smoker` in the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _cross-tabulation_ or _crosstab_ is a special case of pivot table that computes group frequencies (counts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=[tips[\"time\"], tips[\"day\"]], columns=tips[\"smoker\"], margins=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
